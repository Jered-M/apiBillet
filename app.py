import os
import time
import logging
import numpy as np
from flask import Flask, request, jsonify
from flask_cors import CORS
from werkzeug.utils import secure_filename
from werkzeug.exceptions import ClientDisconnected
from PIL import Image, ImageOps

import tensorflow as tf
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input

# Support optionnel pour TFLite
try:
    import tensorflow.lite as tflite
    HAS_TFLITE = True
except:
    HAS_TFLITE = False

# =========================
# CONFIGURATION DU LOGGER
# =========================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# =========================
# CONFIGURATION GLOBALE
# =========================

IMG_SIZE = (224, 224)
UPLOAD_FOLDER = "uploads"
MIN_CONFIDENCE = 0.50

# Labels - 14 classes pour MobileNetV2
BILL_LABELS = {
    0: "1 USD",
    1: "10 USD",
    2: "100 USD",
    3: "10000 CDF",
    4: "1000 CDF",
    5: "100 CDF",
    6: "20 USD",
    7: "20000 CDF",
    8: "200 CDF",
    9: "5 USD",
    10: "50 CDF",
    11: "5000 CDF",
    12: "500 CDF",
    13: "50 CDF",
}

os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# TensorFlow CPU safe
tf.config.set_visible_devices([], "GPU")
tf.config.threading.set_intra_op_parallelism_threads(2)
tf.config.threading.set_inter_op_parallelism_threads(2)

# =========================
# FLASK APP
# =========================

app = Flask(__name__)
app.config["UPLOAD_FOLDER"] = UPLOAD_FOLDER
app.config["MAX_CONTENT_LENGTH"] = 20 * 1024 * 1024  # 20 MB max

CORS(app)

# =========================
# LOAD MODEL
# =========================

MODEL = None
TFLITE_INTERPRETER = None

def load_model_simple():
    """
    Charge et recompile le mod√®le Keras H5 correctement.
    
    La recompilation est CRITIQUE pour garantir:
    - L'optimiseur correct (Adam avec learning_rate=0.0001)
    - La fonction de perte (categorical_crossentropy)
    - Les m√©triques identiques √† l'entra√Ænement
    
    Cela assure la coh√©rence entre Colab et l'API
    """
    global MODEL, TFLITE_INTERPRETER
    
    logger.info("üì¶ Chargement du mod√®le...")
    
    # Charger model.h5 (format Keras HDF5)
    if os.path.exists("model.h5"):
        try:
            logger.info("üìç Chargement: model.h5 (Keras format)")
            MODEL = tf.keras.models.load_model("model.h5")
            logger.info(f"‚úÖ Mod√®le Keras charg√©")
            logger.info(f"  Input shape : {MODEL.input_shape}")
            logger.info(f"  Output shape: {MODEL.output_shape}")
            logger.info(f"  Classes: {MODEL.output_shape[-1]}")
            
            # ===== RECOMPILATION CRITIQUE =====
            # Recompiler avec les M√äMES param√®tres que l'entra√Ænement
            # Cela garantit la coh√©rence avec Colab
            try:
                MODEL.compile(
                    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
                    loss='categorical_crossentropy',
                    metrics=['accuracy']
                )
                logger.info("‚úÖ Mod√®le recompil√© avec Adam(lr=0.0001)")
                logger.info("   ‚úì Loss: categorical_crossentropy")
                logger.info("   ‚úì Metrics: accuracy")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Recompilation impossible (SavedModel?): {e}")
                logger.info("   ‚ÑπÔ∏è  Continuant avec le mod√®le tel quel...")
            
            return True
        except Exception as e:
            logger.error(f"‚ùå Erreur model.h5: {e}", exc_info=True)
            return False
    
    logger.error("‚ùå model.h5 non trouv√© - API non fonctionnelle")
    return False

try:
    if load_model_simple():
        logger.info("üöÄ Mod√®le charg√© et pr√™t pour pr√©dictions!")
    else:
        logger.error("L'API va d√©marrer mais retournera une erreur pour les pr√©dictions")
except Exception as e:
    logger.error(f"‚ùå Erreur au d√©marrage: {e}", exc_info=True)
    MODEL = None
    TFLITE_INTERPRETER = None

# =========================
# IMAGE PREPROCESS (COMPATIBLE MODEL)
# =========================

def preprocess_image(image_path):
    """
    Pr√©traitement IDENTIQUE au Colab pour garantir coh√©rence avec MobileNetV2.

    Points critiques pour la coh√©rence Colab ‚Üî API:
    1. ‚úì Conversion RGB (PIL default)
    2. ‚úì Resize 224x224 avec LANCZOS (ImageDataGenerator default)
    3. ‚úì preprocess_input MobileNetV2 (normalise [-1, 1])
    4. ‚úì Dimension batch [1, 224, 224, 3]
    5. ‚úì Float32 precision (mod√®le attend float32)

    Pipeline:
    1. Charger l'image
    2. Valider le format
    3. Corriger l'orientation EXIF (photos iPhone)
    4. Convertir en RGB
    5. Redimensionner √† 224x224
    6. Appliquer preprocess_input MobileNetV2
    7. Ajouter dimension batch
    """
    try:
        import io
        
        # V√©rifier que le fichier existe et est lisible
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"Fichier non trouv√©: {image_path}")
        
        file_size = os.path.getsize(image_path)
        if file_size == 0:
            raise ValueError(f"Fichier vide: {image_path}")
        
        logger.info(f"üì∏ Ouverture du fichier: {image_path} ({file_size} bytes)")
        
        # Ouvrir et valider l'image
        img = Image.open(image_path)
        img.verify()  # V√©rifier que c'est une image valide
        
        # R√©ouvrir apr√®s verify() (qui ferme le fichier)
        img = Image.open(image_path)
        logger.info(f"‚úÖ Image valide - Format: {img.format}, Size: {img.size}, Mode: {img.mode}")
        
        # ===== √âTAPE 1: CORRECTION EXIF =====
        # Important pour les photos prises avec iPhone qui ont des m√©tadonn√©es EXIF
        img = ImageOps.exif_transpose(img)
        logger.debug(f"  ‚úì EXIF transpos√© - Nouveau size: {img.size}")
        
        # ===== √âTAPE 2: CONVERSION RGB =====
        # ImageDataGenerator convertit automatiquement en RGB
        # C'est CRITIQUE pour coh√©rence avec Colab
        img = img.convert('RGB')
        logger.debug(f"  ‚úì Converti en RGB - Mode: {img.mode}")
        
        # ===== √âTAPE 3: REDIMENSIONNEMENT =====
        # Utiliser LANCZOS (algorithme par d√©faut de PIL pour downsampling)
        # C'est EXACTEMENT ce qu'utilise ImageDataGenerator
        img_resized = img.resize(IMG_SIZE, Image.Resampling.LANCZOS)
        logger.debug(f"  ‚úì Redimensionn√© √† {IMG_SIZE}")
        
        # ===== √âTAPE 4: CONVERSION EN ARRAY =====
        # Float32 (type attendu par le mod√®le Keras)
        img_array = np.array(img_resized, dtype=np.float32)
        logger.debug(f"  ‚úì Converti en array - dtype: {img_array.dtype}, shape: {img_array.shape}")
        
        # ===== √âTAPE 5: NORMALISATION =====
        # Appliquer preprocess_input MobileNetV2 (normalise [-1, 1])
        # Convertit [0, 255] ‚Üí [-1, 1] selon MobileNetV2
        img_array = preprocess_input(img_array)
        logger.debug(f"  ‚úì preprocess_input MobileNetV2 appliqu√© - Range: [{img_array.min():.4f}, {img_array.max():.4f}]")

        # ===== √âTAPE 6: DIMENSION BATCH =====
        # model.predict() attend (batch_size, height, width, channels)
        # Transformer (224, 224, 3) ‚Üí (1, 224, 224, 3)
        img_array = np.expand_dims(img_array, axis=0)
        logger.info(f"‚úÖ Pr√©traitement complet - Shape final: {img_array.shape}")
        logger.info(f"   Data type: {img_array.dtype}, Range: [{img_array.min():.4f}, {img_array.max():.4f}]")
        
        return img_array
        
    except Image.UnidentifiedImageError as e:
        logger.error(f"‚ùå Format image non reconnu: {e}")
        raise ValueError(f"Format image invalide ou corrompu: {str(e)}")
    except FileNotFoundError as e:
        logger.error(f"‚ùå Fichier non trouv√©: {e}")
        raise
    except ValueError as e:
        logger.error(f"‚ùå Fichier vide ou invalide: {e}")
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur inattendue lors du pr√©traitement: {e}", exc_info=True)
        raise ValueError(f"Erreur pr√©traitement: {str(e)}")

# =========================
# INFERENCE FUNCTION
# =========================

def predict_model(img_array):
    """
    Pr√©dit avec le mod√®le Keras H5 (coh√©rent avec Colab).
    
    Le mod√®le.h5 est un mod√®le Keras classique.
    - Input: (1, 224, 224, 3) - array normalis√© [0, 1]
    - Output: (1, 14) - logits pour 14 classes
    - Utilise softmax pour obtenir les probabilit√©s
    """
    try:
        if MODEL is None:
            raise ValueError("Mod√®le non charg√©")
        
        logger.debug(f"üîÆ Input array shape: {img_array.shape}, dtype: {img_array.dtype}")
        
        # ===== PR√âDICTION =====
        # model.predict() retourne les probabilit√©s directement
        # (contrairement √† model(x) qui retourne les logits)
        predictions = MODEL.predict(img_array, verbose=0)
        
        logger.debug(f"  ‚úì Predictions shape: {predictions.shape}")
        logger.debug(f"  ‚úì Predictions sum: {predictions.sum():.4f} (should be ~1.0)")
        
        num_classes = predictions.shape[-1]
        
        # Retourner les pr√©dictions pour la premi√®re image du batch
        # predictions[0] = array de 14 probabilit√©s
        return predictions[0], num_classes
        
    except Exception as e:
        logger.error(f"‚ùå Erreur pr√©diction: {e}", exc_info=True)
        raise ValueError(f"Erreur lors de la pr√©diction: {str(e)}")

# =========================
# ROUTES
# =========================

@app.route("/", methods=["GET"])
def index():
    return "Bill Recognition API running", 200


@app.route("/health", methods=["GET"])
def health():
    model_info = {
        "model_loaded": MODEL is not None,
        "model_type": "SavedModel",
        "source": "model_saved/"
    }
    
    if MODEL is not None:
        try:
            model_info["signatures"] = list(MODEL.signatures.keys())
            model_info["num_classes"] = 14  # MobileNetV2 a 14 classes
            model_info["file"] = "model.h5"
        except:
            pass
    
    is_ready = MODEL is not None
    return jsonify({
        "status": "ok" if is_ready else "model_missing",
        "model": model_info,
        "port": 5000
    }), 200 if is_ready else 503


@app.route("/debug/upload", methods=["POST"])
def debug_upload():
    """Endpoint de debug pour tester les uploads"""
    logger.info("üîç DEBUG: Request re√ßue")
    logger.info(f"  Content-Type: {request.content_type}")
    logger.info(f"  Form keys: {list(request.form.keys())}")
    logger.info(f"  Files keys: {list(request.files.keys())}")
    logger.info(f"  Args keys: {list(request.args.keys())}")
    
    if "file" in request.files:
        file = request.files["file"]
        logger.info(f"  File name: {file.filename}")
        logger.info(f"  File size: {len(file.read())} bytes")
        file.seek(0)
        return jsonify({
            "debug": "File re√ßu avec succ√®s",
            "filename": file.filename,
            "size": len(file.read())
        }), 200
    else:
        return jsonify({
            "error": "Pas de fichier d√©tect√©",
            "files_keys": list(request.files.keys()),
            "content_type": request.content_type
        }), 400


@app.route("/debug/save-raw", methods=["POST"])
def debug_save_raw():
    """Sauvegarde l'image brute SANS preprocessing pour test Colab"""
    logger.info("üíæ DEBUG: Sauvegarde image brute pour test")
    
    if "file" not in request.files:
        return jsonify({"error": "Pas de fichier"}), 400
    
    file = request.files["file"]
    if file.filename == "":
        return jsonify({"error": "Nom vide"}), 400
    
    filename = secure_filename(file.filename)
    filepath = os.path.join(app.config["UPLOAD_FOLDER"], f"raw_{filename}")
    
    try:
        file.save(filepath)
        logger.info(f"‚úÖ Image brute sauvegard√©e: {filepath}")
        return jsonify({
            "message": "Image sauvegard√©e",
            "path": filepath,
            "instruction": "T√©l√©charge cette image et teste dans Colab"
        }), 200
    except Exception as e:
        logger.error(f"‚ùå Erreur: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/predict", methods=["POST"])
def predict():
    """
    Endpoint de pr√©diction - Robuste avec validation compl√®te
    
    Logique:
    1. Valide la pr√©sence et le format du fichier
    2. Pr√©traite (redimensionner 224x224, normaliser /255)
    3. Pr√©dit la classe
    4. Retourne la classe et la confiance
    """
    start_time = time.time()
    filepath = None
    
    try:
        # ===== VALIDATION DU MOD√àLE =====
        if TFLITE_INTERPRETER is None and MODEL is None:
            logger.error("‚ùå Aucun mod√®le disponible")
            return jsonify({
                "error": "Mod√®le non charg√©",
                "status": "model_missing"
            }), 503

        # ===== VALIDATION DE LA REQU√äTE =====
        logger.debug(f"üîç Content-Type: {request.content_type}")
        try:
            logger.debug(f"üîç Files keys: {list(request.files.keys())}")
        except ClientDisconnected:
            logger.warning("‚ö†Ô∏è  Client d√©connect√© lors du traitement de la requ√™te")
            return jsonify({
                "error": "Client d√©connect√© - veuillez r√©essayer"
            }), 400
        
        try:
            files_keys = list(request.files.keys())
        except ClientDisconnected:
            logger.warning("‚ö†Ô∏è  Client d√©connect√© lors de l'acc√®s √† request.files")
            return jsonify({
                "error": "Client d√©connect√© - veuillez r√©essayer"
            }), 400
        
        if "file" not in request.files:
            logger.warning("‚ö†Ô∏è  Cl√© 'file' manquante dans request.files")
            return jsonify({
                "error": "Cl√© 'file' manquante. Utilisez: files={'file': open('image.jpg', 'rb')}",
                "received_keys": files_keys,
                "content_type": request.content_type
            }), 400

        try:
            file = request.files["file"]
        except ClientDisconnected:
            logger.warning("‚ö†Ô∏è  Client d√©connect√© lors de la r√©cup√©ration du fichier")
            return jsonify({
                "error": "Client d√©connect√© - veuillez r√©essayer"
            }), 400
        
        if file.filename == "":
            logger.warning("‚ö†Ô∏è  Filename vide")
            return jsonify({
                "error": "Filename vide - impossible de traiter"
            }), 400

        # ===== VALIDATION DE L'EXTENSION =====
        filename = file.filename
        if "." not in filename:
            logger.warning(f"‚ö†Ô∏è  Extension manquante: {filename}")
            return jsonify({
                "error": f"Extension manquante. Formats accept√©s: jpg, jpeg, png, gif, bmp"
            }), 400
        
        ext = filename.rsplit(".", 1)[-1].lower()
        allowed_extensions = {"jpg", "jpeg", "png", "gif", "bmp"}
        
        if ext not in allowed_extensions:
            logger.warning(f"‚ö†Ô∏è  Format non support√©: {ext}")
            return jsonify({
                "error": f"Format '{ext}' non support√©",
                "allowed_formats": list(allowed_extensions)
            }), 400

        # ===== SAUVEGARDE TEMPORAIRE =====
        filename_safe = secure_filename(filename)
        # Ajouter timestamp pour √©viter les collisions
        import uuid
        filename_unique = f"{uuid.uuid4().hex}_{filename_safe}"
        filepath = os.path.join(app.config["UPLOAD_FOLDER"], filename_unique)
        
        file.save(filepath)
        file_size = os.path.getsize(filepath)
        logger.info(f"‚úÖ Fichier sauvegard√©: {filename_unique} ({file_size} bytes)")
        
        # V√©rifier que le fichier a bien √©t√© √©crit
        if file_size == 0:
            logger.error("‚ùå Fichier vide apr√®s la sauvegarde")
            return jsonify({
                "error": "Fichier vide - impossible de traiter"
            }), 400
        
        # ===== PR√âTRAITEMENT =====
        try:
            img_array = preprocess_image(filepath)
            logger.info(f"‚úÖ Image pr√©trait√©e - Shape: {img_array.shape}")
        except ValueError as e:
            logger.error(f"‚ùå Erreur pr√©traitement (ValueError): {e}")
            return jsonify({
                "error": f"Image invalide: {str(e)}"
            }), 400
        except Exception as e:
            logger.error(f"‚ùå Erreur pr√©traitement (Exception): {e}")
            return jsonify({
                "error": f"Erreur traitement image: {str(e)}"
            }), 500
        
        # ===== PR√âDICTION =====
        try:
            logger.info("üîÆ Pr√©diction en cours avec model.h5...")
            predictions, num_classes = predict_model(img_array)
            logger.info(f"‚úÖ Pr√©diction r√©ussie - {num_classes} classes d√©tect√©es")
        except ValueError as e:
            logger.error(f"‚ùå Erreur pr√©diction (ValueError): {e}")
            return jsonify({
                "error": f"Erreur pr√©diction: {str(e)}"
            }), 500
        except Exception as e:
            logger.error(f"‚ùå Erreur pr√©diction (Exception): {e}")
            return jsonify({
                "error": f"Erreur serveur pr√©diction: {str(e)}"
            }), 500
        
        # ===== ANALYSE DES R√âSULTATS =====
        predicted_class_idx = int(np.argmax(predictions))
        confidence = float(predictions[predicted_class_idx])
        num_classes = int(num_classes)
        
        logger.debug(f"  ‚úì Classe pr√©dite: {predicted_class_idx}")
        logger.debug(f"  ‚úì Confiance: {confidence:.4f}")
        logger.debug(f"  ‚úì Top 3 pr√©dictions:")
        top_3_idx = np.argsort(predictions)[::-1][:3]
        for i, idx in enumerate(top_3_idx):
            logger.debug(f"    {i+1}. Classe {idx}: {predictions[idx]:.4f} ({BILL_LABELS.get(idx, '?')})")
        
        # ===== R√âCUP√âRATION DU LABEL =====
        predicted_label = BILL_LABELS.get(predicted_class_idx, f"Unknown ({predicted_class_idx})")
        
        logger.info(f"üéØ R√âSULTAT FINAL: {predicted_label} ({confidence*100:.2f}%) [Classe {predicted_class_idx}/{num_classes}]")
        
        # ===== R√âPONSE JSON =====
        # Format coh√©rent avec les attentes de l'app mobile
        response = {
            "result": predicted_label,
            "prediction": predicted_label,
            "confidence": float(confidence),
            "confidence_percent": round(float(confidence) * 100, 2),
            "class": int(predicted_class_idx),
            "class_index": int(predicted_class_idx),
            "num_classes": num_classes,
            "model": "model.h5 (Keras)",
            "model_source": "Colab training",
            "processing_time_ms": round((time.time() - start_time) * 1000, 2)
        }
        
        logger.info(f"‚úÖ R√©ponse pr√©par√©e: {response}")
        return jsonify(response), 200
        
    except FileNotFoundError as e:
        logger.error(f"‚ùå Fichier non trouv√©: {e}")
        return jsonify({
            "error": f"Erreur fichier: {str(e)}",
            "error_type": "file_not_found"
        }), 500
    except ValueError as e:
        logger.error(f"‚ùå Erreur validation: {e}")
        return jsonify({
            "error": f"Format image invalide: {str(e)}",
            "error_type": "invalid_image"
        }), 400
    except Exception as e:
        logger.error(f"‚ùå Erreur serveur: {e}", exc_info=True)
        return jsonify({
            "error": f"Erreur serveur: {str(e)}",
            "error_type": "server_error"
        }), 500
    
    finally:
        # ===== NETTOYAGE =====
        if filepath and os.path.exists(filepath):
            try:
                os.remove(filepath)
                logger.debug(f"üóëÔ∏è  Fichier temporaire supprim√©: {filepath}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Impossible de supprimer {filepath}: {e}")

# =========================
# MAIN
# =========================

if __name__ == "__main__":
    app.run(
        host="0.0.0.0",
        port=5000,
        debug=False,
        threaded=True
    )
