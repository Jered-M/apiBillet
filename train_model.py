"""
Script d'entra√Ænement du mod√®le Bill Recognition
Adapt√© du code Colab pour fonctionner localement
"""

import os
import numpy as np
import pandas as pd
from PIL import Image, ImageFile
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Model

import logging

# =========================
# CONFIGURATION
# =========================

IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32
EPOCHS = 20
DATASET_PATH = "./dataset"  # Adapter le chemin selon votre structure
MODEL_SAVE_PATH = "model (1).h5"

# Autoriser les images tronqu√©es
ImageFile.LOAD_TRUNCATED_IMAGES = True

# Configuration TensorFlow
tf.config.set_visible_devices([], 'GPU')
tf.config.threading.set_inter_op_parallelism_threads(2)
tf.config.threading.set_intra_op_parallelism_threads(2)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("ModelTraining")

# =========================
# √âTAPE 1: SCANNER LE DATASET
# =========================

def get_dataset_structure(dataset_path):
    """V√©rifie et liste la structure du dataset"""
    if not os.path.exists(dataset_path):
        logger.error(f"‚ùå Dataset non trouv√©: {dataset_path}")
        return None
    
    classes = sorted([d for d in os.listdir(dataset_path) 
                     if os.path.isdir(os.path.join(dataset_path, d))])
    
    if not classes:
        logger.error(f"‚ùå Aucune classe trouv√©e dans {dataset_path}")
        return None
    
    logger.info(f"‚úÖ {len(classes)} classes trouv√©es:")
    for i, cls in enumerate(classes, 1):
        class_path = os.path.join(dataset_path, cls)
        num_images = len([f for f in os.listdir(class_path) 
                         if os.path.isfile(os.path.join(class_path, f))])
        logger.info(f"  {i}. {cls}: {num_images} images")
    
    return classes

# =========================
# √âTAPE 2: SCANNER LES IMAGES VALIDES
# =========================

def scan_valid_images(dataset_path, classes):
    """Scanne et recense les images valides"""
    valid_images = []
    corrupted_images = []
    
    logger.info("\nüì∏ Scanning des images...")
    
    for class_name in classes:
        class_path = os.path.join(dataset_path, class_name)
        
        for img_name in os.listdir(class_path):
            img_path = os.path.join(class_path, img_name)
            
            if not os.path.isfile(img_path):
                continue
            
            try:
                with Image.open(img_path) as img:
                    img.verify()
                valid_images.append({'path': img_path, 'class': class_name})
            except Exception as e:
                corrupted_images.append({'path': img_path, 'error': str(e)})
    
    logger.info(f"‚úÖ {len(valid_images)} images valides trouv√©es")
    if corrupted_images:
        logger.warning(f"‚ö†Ô∏è {len(corrupted_images)} images corrompues d√©tect√©es")
    
    return valid_images

# =========================
# √âTAPE 3: CR√âER LES DATA GENERATORS
# =========================

def create_data_generators(valid_images):
    """Cr√©e les g√©n√©rateurs de donn√©es train/validation"""
    logger.info("\nüîÑ Cr√©ation des data generators...")
    
    # Cr√©er un DataFrame
    valid_df = pd.DataFrame(valid_images)
    
    # Split 80/20
    train_df, validation_df = train_test_split(
        valid_df,
        test_size=0.2,
        random_state=42,
        stratify=valid_df['class']
    )
    
    logger.info(f"‚úÖ Training: {len(train_df)} images")
    logger.info(f"‚úÖ Validation: {len(validation_df)} images")
    
    # Data augmentation pour training
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=40,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True
    )
    
    # Pas d'augmentation pour validation
    val_datagen = ImageDataGenerator(rescale=1./255)
    
    # Cr√©er les g√©n√©rateurs
    train_generator = train_datagen.flow_from_dataframe(
        dataframe=train_df,
        x_col='path',
        y_col='class',
        target_size=(IMG_HEIGHT, IMG_WIDTH),
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        shuffle=True
    )
    
    validation_generator = val_datagen.flow_from_dataframe(
        dataframe=validation_df,
        x_col='path',
        y_col='class',
        target_size=(IMG_HEIGHT, IMG_WIDTH),
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        shuffle=False
    )
    
    return train_generator, validation_generator

# =========================
# √âTAPE 4: CONSTRUIRE LE MOD√àLE
# =========================

def build_model(num_classes):
    """Construit le mod√®le avec transfer learning MobileNetV2"""
    logger.info("\nüèóÔ∏è Construction du mod√®le...")
    
    # Charger MobileNetV2 pr√©-entra√Æn√©
    base_model = MobileNetV2(
        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),
        include_top=False,
        weights='imagenet'
    )
    
    # Geler les poids du base model
    base_model.trainable = False
    logger.info("‚úÖ MobileNetV2 charg√© et gel√©")
    
    # Construire le mod√®le personnalis√©
    inputs = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))
    x = base_model(inputs, training=False)
    x = GlobalAveragePooling2D()(x)
    x = Dense(256, activation='relu')(x)
    outputs = Dense(num_classes, activation='softmax')(x)
    
    model = Model(inputs, outputs)
    
    # D√©geler les derni√®res couches pour fine-tuning
    fine_tune_at = -30
    for layer in base_model.layers[fine_tune_at:]:
        if not isinstance(layer, tf.keras.layers.BatchNormalization):
            layer.trainable = True
    
    logger.info(f"‚úÖ Mod√®le construit ({len(model.layers)} couches)")
    
    return model

# =========================
# √âTAPE 5: COMPILER ET ENTRA√éNER
# =========================

def train_model(model, train_generator, validation_generator):
    """Entra√Æne le mod√®le"""
    logger.info("\n‚öôÔ∏è Compilation du mod√®le...")
    
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    logger.info("‚úÖ Mod√®le compil√©")
    logger.info(f"\nüöÄ D√©marrage de l'entra√Ænement ({EPOCHS} epochs)...\n")
    
    history = model.fit(
        train_generator,
        epochs=EPOCHS,
        validation_data=validation_generator,
        verbose=1
    )
    
    logger.info("\n‚úÖ Entra√Ænement termin√©")
    return history

# =========================
# √âTAPE 6: √âVALUER ET SAUVEGARDER
# =========================

def evaluate_model(model, validation_generator):
    """√âvalue le mod√®le"""
    logger.info("\nüìä √âvaluation du mod√®le...")
    
    validation_generator.reset()
    Y_true = validation_generator.classes
    
    Y_pred_probs = model.predict(validation_generator, verbose=1)
    Y_pred = np.argmax(Y_pred_probs, axis=1)
    
    class_labels = list(validation_generator.class_indices.keys())
    
    # Confusion matrix
    cm = confusion_matrix(Y_true, Y_pred)
    
    # Classification report
    report = classification_report(Y_true, Y_pred, target_names=class_labels)
    
    logger.info("\n" + "="*60)
    logger.info("CLASSIFICATION REPORT")
    logger.info("="*60)
    logger.info(report)
    
    return cm, report

def save_model(model, save_path):
    """Sauvegarde le mod√®le"""
    logger.info(f"\nüíæ Sauvegarde du mod√®le √† {save_path}...")
    model.save(save_path)
    logger.info("‚úÖ Mod√®le sauvegard√©")

# =========================
# MAIN
# =========================

def main():
    logger.info("="*60)
    logger.info("BILL RECOGNITION - ENTRA√éNEMENT DU MOD√àLE")
    logger.info("="*60)
    
    # √âtape 1: V√©rifier le dataset
    logger.info(f"\nüìÅ Recherche du dataset: {DATASET_PATH}")
    classes = get_dataset_structure(DATASET_PATH)
    if not classes:
        return
    
    # √âtape 2: Scanner les images
    valid_images = scan_valid_images(DATASET_PATH, classes)
    if not valid_images:
        logger.error("‚ùå Aucune image valide trouv√©e")
        return
    
    # √âtape 3: Cr√©er les generators
    train_gen, val_gen = create_data_generators(valid_images)
    
    # √âtape 4: Construire le mod√®le
    model = build_model(len(classes))
    
    # √âtape 5: Entra√Æner
    history = train_model(model, train_gen, val_gen)
    
    # √âtape 6: √âvaluer et sauvegarder
    evaluate_model(model, val_gen)
    save_model(model, MODEL_SAVE_PATH)
    
    logger.info("\n" + "="*60)
    logger.info("‚úÖ PROCESSUS TERMIN√â AVEC SUCC√àS")
    logger.info("="*60)
    logger.info(f"üì¶ Mod√®le disponible √†: {MODEL_SAVE_PATH}")

if __name__ == "__main__":
    main()
